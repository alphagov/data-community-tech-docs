---
title: Use the forward path tool
weight: 39.1
last_reviewed_on: 2022-02-23
review_in: 6 months
---

# Use the forward path tool

The forward path tool shows the pages a user visits after visiting a page of interest on GOV.UK.

This tool outputs a CSV file with the count and proportion of user sessions visiting distinct, subsetted journeys for a specified date range. This list of sessions is in the CSV file as a sorted list in descending order, categorised by device.

___Tried to rephrase this but not sure - what does distinct subsetted journeys mean?___

To use the forward path tool, you must do the following.

1. Download the forward path tool notebook.
1. Open the tool notebook in Google Colab.
1. Run the notebook.

## Download the forward path tool

___Do you need any specific permission to do this?___

Download the notebook from GitHub.

1. Go to the [`govuk-user-journey-analysis-tools` GitHub repo][repository].
1. Select the __Code__ button, and then select __Download Zip__.
1. Unzip the __govuk-user-journey-analysis-tools__ folder and go to the __notebooks__ folder.
1. Save the __forward-path-tool__ Jupyter notebook to your Google Drive account.

## Open the tool notebook in Google Colab

1. Go to [Google Colab](https://colab.research.google.com/). You will see an window to open a notebook.
1. Select the __Google Drive__ tab and open the __forward-path-tool__ notebook.

___Cannot find below___

You may need to 'connect' the notebook to be associated with Google Colab the first time you open a Jupyter
notebook from Google Drive. To do this, look for the link at the top-centre of the page which says something
like `Connect more apps` (see screenshot below), search for `Google Colaboratory` and then accept any required
permissions it asks for. Once you have associated Jupyter notebooks with Google Colab, the notebook should open.

## Run the tool notebook

To run the notebook, you must do the following.

1. Authenticate your access.
1. Set the query parameters.

### Authenticate your access

1. Hover your cursor over the cell that starts with the code `from datetime import datetime` to show the run icon. Select the run icon to start the authentication process.
1. Select the authentication link in the text box and then select your Google account.
1. Follow the on screen prompts, selecting __Allow__ when prompted, and copy the __Sign in code__ when this code appears.
1. Go back to the text box in the notebook and paste the sign in code into the __Enter verification code__ field.
1. Select __Enter__ to complete authentication.

If you receive a warning message saying "The notebook was not authored by Google", select __Run Anyway__.

### Set the query parameters

You set the query parameters in the __Set query parameters__ cell.

1. You must set the following mandatory query parameters:
  - start and end dates for the analysis
  - desired page path of interest
  - the number of pages or events the user journeys will be subsetted by
  - whether to include page and/or event hits
  - device categories to include

1. You can set the following optional query parameters on whether to:
  - remove query strings from the page path of interest
  - append event-associated page paths with an __[E]__,
  - append event-associated page paths with the event category, event action, and/or event label suffixes
  - flag journeys that include the entrance page
  - remove refreshes of the page of interest
  - have search pages only show the search content type and search keywords

Once you are happy with the query parameters, select the cell and first select __Runtime__, and then __Run after__. The notebook will run each cell following the __Set query parameters__ cell one at a time. ___what does this mean??___

___cannot see this option on the notebook?___

### Execute the query and save output

The notebook should automatically scroll down to __cell 5__. Manually scroll to the cell if you need to.

___which is cell 5??? Does it start with "Initialise a Google BigQuery client, and define the query parameters" ??___

This cell estimates the number of gigabytes read by the query and shows you the amount.

If you are happy to run the query, enter "yes" into the user input box.

If you leave the input box blank or type in something other than "yes", the query will not run.

The query generates the requested data, creates a local CSV file and exports the data into that file.

___Where is the CSV file? https://drive.google.com/drive/folders/1DBXzHbg5qLiFYK8BIuYGKFjlDBkQALUs ?? Or somewhere else?___

If no CSV file is created, check the end of the URL search bar. If you see a download icon with a red cross, select the icon and change the option to __Always allow...__, and then select __Done__.

The CSV file shows the top 10 forward path tool results, and follows the [forward path template](https://docs.google.com/spreadsheets/d/1kISyKu2jVzINCxwPe8ydQM8cibgEX2a3WCPxkJM9W80/edit#gid=1115034830). To access this template, [email the GOV.UK Data Labs team](mailto:gov.uk-data-labs@digital.cabinet-office.gov.uk).

[assumptions]: ../aqa/assumptions-caveats-forward-path-tool.md
[repository]: https://github.com/alphagov/govuk-user-journey-analysis-tools

## Assumptions and caveats

This log contains a list of assumptions and caveats used in the Forward Path tool analysis.

### Definitions

Assumptions are RAG-rated according to the following definitions for quality and impact.

| RAG   | Assumption quality | Assumption impact |
|---|---|---|
|Green|Reliable assumption, well understood and/or documented; anything up to a validated & recent set of actual data.|Marginal assumptions; their changes have no or limited impact on the outputs.|
|Amber|Some evidence to support the assumption; may vary from a source with poor methodology to a good source that is a few years old.|Assumptions with a relevant, even if not critical, impact on the outputs.|
|Red|Little evidence to support the assumption; may vary from an opinion to a limited data source with poor methodology|Core assumptions of the analysis; the output would be drastically affected by their change.|

Thank you to the Home Office Analytical Quality Assurance team for these definitions.

### Assumption 1: Only exact matches to `DESIRed_PAGE` are currently supported

* Quality: Green
* Impact: Amber

The query parameter for `DESIRed_PAGE` cannot evaluate the field using regular expression, and therefore only exact matches are currently supported. This is acceptable as users of these tools are accustomed to providing exact matches for analyses.

### Assumption 2: Previous visits to `DESIRed_PAGE` are ignored, only the last visit is used

* Quality: Green
* Impact: Amber

The subsetted journey considers the last visit to the `DESIRed_PAGE` as the goal location (that is, the first step).

Therefore, any previous visits to the `DESIRed_PAGE` are ignored. This was decided as the main aim of the tool is to understand which pages are visited following the `DESIRed_PAGE`.

However, it is important that the user of the tool is aware of this assumption, as it will impact the subsetted journey output.

### Assumption 3: If `REMOVE_DESIRed_PAGE_REFRESHES` is `TRUE`, only the first visit in a series of sequential visits (page refreshes) to `DESIRed_PAGE` are used to determine which is the last visit.

* Quality: Green
* Impact: Green

Therefore, if `REMOVE_DESIRed_PAGE_REFRESHES` is `TRUE`, it will only use the first visit in a series of sequential visits to `DESIRed_PAGE` of hit type PAGE. Other earlier visits to `DESIRed_PAGE` will remain, as will any earlier desired page refreshes.

### Assumption 4: Journeys shorter than the number of desired stages (`NUMBER_OF_STAGES`) are always included.

* Quality: Green
* Impact: Amber

While journeys shorter than the number of desired stages are always included, journeys longer than the number of desired stages will not be accurately represented.

For example, if `NUMBER_OF_STAGES = 2`, and the journey consists of 3 stages, then the user of the tool will not be provided with the full journey (that is, the 3rd page path).

Therefore, this journey will be amalgamated with other journeys that consist of the same 2 page paths (`NUMBER_OF_STAGES = 2`), even if further page paths differ.

### Assumption 5: GOV.UK search page paths are assumed to have the format /search/{TYPE}?keywords={KEYWORDS}{...}

* Quality: Green
* Impact: Green

`{TYPE}` is the GOV.UK search content type, `{KEYWORDS}` are the search keywords, where each keyword is separated by +, and {...} are any other parts of the search query that are not keyword-related (if they exist).

### Assumption 6: GOV.UK search page titles are assumed to have the format {KEYWORDS} - {TYPE} - GOV.UK

* Quality: Green
* Impact: Green

`{TYPE}` is the GOV.UK search content type, and `{KEYWORDS}` are the search keywords.

### Assumption 7: If `ENTRANCE_PAGE` is `FALSE`, each journey contains both instances where the entrance page is included, and the entrance page is not included.

* Quality: Green
* Impact: Red

Therefore, if `ENTRANCE_PAGE` is `TRUE`, these two instances (where the entrance page is included against when the entrance page is not included) will be considered two separate journeys.

This is a better representation of the journey, as a `TRUE` flag indicates that the journey had more page paths than `NUMBER_OF_STAGES`.

The user must have a good understanding of what the `ENTRANCE_FLAG` represents, as this could drastically change the output.

### Assumption 8: If `DEVICE_ALL` is selected in combination with either `DEVICE_DESKTOP`, `DEVICE_MOBILE`, and/or `DEVICE_TABLET`, then the analysis will use `DEVICE_ALL` and ignore all other arguments.

* Quality: Green
* Impact: Red

Be default, `DEVICE_ALL` argument will be implemented over `DEVICE_DESKTOP`, `DEVICE_MOBILE`, and `DEVICE_TABLET`.

Therefore, if the user accidentally selects `DEVICE_ALL`, the query will ignore the desired arguments `DEVICE_DESKTOP`, `DEVICE_MOBILE`, and/or `DEVICE_TABLET`.

This will drastically change the expected output, as `DEVICE_ALL` will split up journeys undertaken on a desktop, mobile, and tablet device.

The output CSV file flags which device category(ies) were used, which should mitigate errors related to interpreting the data.
