---
title: Use the IUNIS algorithm
weight: 39.61
last_reviewed_on: 2022-03-04
review_in: 6 months
---

# Use the IUNIS algorithm

In the Inferring User Needs by Information Scents (IUNIS) algorithm, the list of keywords that represent the user journeyâ€™s intent are driven by the:

- pages that a user visits in their journey
- possible pages that they could have visited but did not,

The algorithm weights pages according to how rare those pages are across all user journeys in the specified time period.

The algorithm then weights the derived keywords by how important and unique those words are to the weighted pages compared to the rest of GOV.UK. Only the top-weighted keywords are kept.

To use IUNIS, you must create the:

- page term TF-IDF matrix
- page-access weight matrix
- step function alpha vector
- topology matrix

Then, run the IUNIS algorithm on the step-alpha function.

## Create the page term TF-IDF matrix

___What is this thing????___

To create the page term TF-IDF matrix:

- set the env var
- execute the Jupyter notebooks

If you are using AWS Sagemaker (link) you must also upload the copy of the GOV.UK mirror to your jupyter instance
___how do i do this???___
___what else would i use instead of sagemaker???___

### Set the env var

In the .envrc file, set the DIR_MIRROR environment variable using the folder path to your downloaded GOV.UK mirror folder;
link to loading env var

### Execute the Jupyter notebooks

If using AWS Sagemaker, please first run: startup_script.ipynb and make sure the
file notebooks/page_term_tfidf_matrix/setup_script.py is in the specified directory
___what specified directory? if it isnt do i just copy it from the intent detector repo???___

At the root-level of this repository, run `jupyter notebook` to open Jupyter in your terminal.
___the intent detector repository???___

On the Jupyter interface, navigate to the `notebooks/page_term_tfidf_matrix`.
There are nine Jupyter notebooks here.

To produce a page-term TF-IDF matrix for the whole GOV.UK mirror, run the notebooks in the following order.

modify the DATA_DATE in each notebook before you run them

Open each notebook in turn, and execute all the cells -

002. Get the filepaths to each, and every HTML file from the GOV.UK mirror; output stored in `data/interim` folder
___where is that folder???___

003b. [only for PoC set of curated user journeys]
Compare the outputs from Step 002 and the page urls for the PoC curated user journeys in the yaml file.
In case any page is missing, web-scraping GOV.UK following notebook 003 below
(there was none for the PoC first iteration);
___what is PoC???___
scraped HTML pages stored in the `data/raw/scraped` folder
updated index of filepaths stored in the `data/interim` folder
___where is this folder???___

004b. Parse all the valid HTML pages from Step 002 to show only visible text;
Remove irrelevant html parts (e.g., header, banner)
output stored in `data/interim` folder

005. Set the outputs from Step 004 to lowercase, remove digits and otehr non-alpha symbols,
lemmatise them using WordNet parts of speech tags, and remove English stopwords; and
___what does lemmatise mean??? sort so as to group together inflected or variant forms of the same word___
output stored in `data/interim` folder

006. Generate the TF-IDF matrix.
Outputs are stored in the data/processed folder

## Create the page-access weight matrix

The user-page access TF-IDF weight matrix aims to give less weight to pages that are accessed by many users and are not relevant to the user's information needs. For example, the GOV.UK homepage.

### Definitions

the "user" is defined as either "sessionId" or "fullVisitorId";
at the moment, the "pages" are limited to the ones in the curated set of user journeys ("target urls");
the "term frequency" (TF) corresponds to the access frequency of the page by the given user;
the "inverse document frequency" (IDF) is the ratio of total users to the number of distinct users who have accessed the given page.

### Create the matrix

The script requires the following two parametrised sql queries:

src/make_data/queries/session_page_access_matrix.sql
src/make_data/queries/visitor_page_access_matrix.sql

___what does a user do with these??? run them before the script??? where???___

Run the python script `src.make_data.make_user_page_access_matrix` to create the user-page access matrix.
___where is this???___
___where do i run the script?___

Specify the following positional arguments in the script:

--which-user:        (optional, default='sessionId') whether the user in the user-page access matrix is
                      the 'sessionId' or the 'fullVisitorId'
--start_date         (required) start date to query as "YYYYMMDD" string (e.g., "20210428)
--end_date:          (required) start date to query as "YYYYMMDD" string (e.g., "20210428)
--tfidf-log:         (optional, default=1 : 'yes') whether to use the log(IDF) in the TF-IDF calculation

For example, to create a visitor-page TF-IDF access matrix for 28-April-2021 using the raw IDF, go to the root-level of this repository, open your terminal and run

python -m src.make_data.make_user_page_access_matrix --start-date "20210428" \
     --end-date "20210428" --which-user "fullVisitorId" --tfidf-log 0


Outputs
The script outputs two .parquet files:

the "raw" output table of the sql query, saved in the data/interim/ folder as {start_date>}_{end_date}_{chosen_user}_page_access_raw_table.parquet
the user-target page access TF-IDF matrix, saved in the data/interim/ folder as {start_date>}_{end_date}_{chosen_user}_{tfidf_log}_page_access_matrix.parquet

___where are these folders? local machine? any setup of folder structure required?___

Lines 48-55 in src/make_data/queries/session_page_access_matrix.sql are only required for the artificial set of user journeys used for the PoC. To be removed when scaling the algorithm.
___what does this mean???___

## Create the step function alpha vector

The script produces an alpha dampening parameter for the pages visited within a specified time window, on all of GOV.UK. This is a vector, beginning at 100%, and decreasing thereafter, for example 40%, then 20%. Each element represents the percentage of sessions that were n pages long or longer, so 100% of sessions were 1 page long or longer, 40% were two pages long or longer, etc.

The vector that is actually used is a modification of that. When the elements of alpha are applied, one after the other, their effect is the product of all the elements applied so far. In the example, the vector that would be used would be 100%, then 40%, then 50%, so that the cumulative products of that vector would be 100%, 40%, and 20%, which are the percentage of sessions that were 1 page long or longer, two pages long or longer, etc.

=n pages long | % of sessions | Alpha | Cumulative product of alpha ---------------|---------------|-------|---------------------------- 1 | 100% | 100 | 100% 2 | 40% | 40% | 40% 3 | 20% | 50% | 20%

___i have no idea what this means___

### Definitions

The alpha dampening parameter is one of the component of the WUFIS and IUNIS algorithms to predict journeys and to infer intents. It makes more sense in WUFIS, as a kind of stopping rule for predicting a journey; if a journey is already longer than most journeys on GOV.UK, then a user is unlikely to follow any further links, nomatter how relevant -- therefore downweight those pages.

Given all sessions on GOV.UK between two dates, create a vector where the first element is the percentage of sessions 1-or-more pages long, the second element is the percentage of sessions 2-or-more pages long, and the nth element is the percentage of sessions n-or-more pages long.

___this is not a definition - what is this supposed to say???___

### Create the vector

Before you run script...

The script requires the following parametrised sql query:

src/make_data/queries/calc_num_pages_visited_per_session.sql

___what does this mean???___

Run the python script `src.make_data.make_alpha_step_function` to create the step-function alpha-values vector.

___where???___

The script allows for the following positional arguments:

Args:

``--alpha-start-date``: (required) start date to query as "YYYYMMDD" string (e.g "20210428")
``--alpha-end-date``:   (required) start date to query as "YYYYMMDD" string (e.g., "20210428")
For instance, to calculate the alpha vector for the PoC curated-jouneys pages based on sessions between 28-April-2021 to 04-May-2021 run in the root directory:

    python -m src.make_data.make_alpha_step_function --alpha-start-date "20210428" --alpha-end-date "20210428"

Outputs

Running the script => following output

A .parquet file data/interim/{START_DATE}_{END_DATE}_alpha_step_function.parquet, with the output table of the sql query. The table has five columns:

___where does this output to???___

numberPagesVisited integer; how many pages were visited in a session
frequencyPageCount integer; the number of sessions that visited that many pages
percentagePageCount float; the percentage of sessions that visited that many pages
cumulativePercentagePageCount float; the percentage of sessions that visited at least that many pages
relativePercentagePageCount float; of sessions that visited at least numberpagesVisited - 1, the percentage that also visited numberPagesVisited. When numberPagesVisited is 1, relativePercentagePageCount is also 1.

## Create the topology matrix

The topology matrix, also known as the directed adjacency matrix, represents how pages are connected to each other through hyperlink.

Two versions of the topology matrix have been generated:

For the pages in the curated set of user journeys.
For all the pages in the GOV.UK mirror.
The version for the curated set of journeys treats URLs with anchors as pages in their own right (an approach that is now deprecated), whereas the version for the GOV.UK mirror ignores anchors. See the assumptions and caveats.

___which version are we using now???___

### Create the matrix (curated journeys)

At the root-level of this repository, open Jupyter in your terminal.

jupyter notebook


The notebook requires:

an internet connection
___do we really need to specify this???___
a .yaml file containing a list of "curated" GOV.UK page urls saved in `data/external/curated_journey_urls.yaml`
___where is this???___
The .yaml file should have a single key, target_urls, with the list of "curated" GOV.UK pages as its value.

For example, if the only curated pages are the GOV.UK homepage, and the main coronavirus page, the yaml file is:

```
target_urls:
  - /
  - /coronavirus
.
```

The notebook consists of two steps:

For the pages specified in the curated user-journey set (see .yaml file), download the (current time) GOV.UK HTML pages locally. For any pages specified with an anchor heading, the code automatically strips out all HTML content prior to this heading.

Generate the directed adjacency (topology) matrix and save it locally as a .pickle file to the data/interim/ folder.

___what am I doing and how am I doing it???___

Outputs
The HTML pages will be saved locally in the data/raw/html folder.
The topology matrix will be saved in the data/interim/ folder as {YYYYMMDD}_{HHMMSS}_govuk_topology_matrix.pickle

### Create the matrix (GOV.UK Mirror)

At the root-level of this repository, run the following command.

python -m src.make_data.generate_topology_matrix
Requirements
A copy of the GOV.UK production mirror.

Outputs
Three files will be saved locally in the data/interim folder.

{YYYYMMDD}_{HHMMSS}_govuk_link_list.pickle") A dictionary keyed by URL, where each item contains a list of other URLs that are linked to.
{YYYYMMDD}_{HHMMSS}_govuk_topology_matrix.pickle") A scipy sparse matrix. The first dimension is the 'from' URL, and the second dimension is the 'to' URL.
{YYYYMMDD}_{HHMMSS}_govuk_vertex_url.pickle") A vector of URLs in the same order as each dimension of the topology matrix. This is used to slice the topology matrix for URLs of interest.
Slicing the topology matrix for a given URL
See commented code at the bottom of notebooks/generate_topology_matrix.py for an example.

## Run the IUNIS algorithm on the step-alpha function.

https://github.com/alphagov/govuk-intent-detector/blob/main/notebooks/proof_of_concept_alpha_step_function.py

___what am i doing and how am i doing it???___
