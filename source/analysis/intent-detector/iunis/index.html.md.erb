---
title: Use the IUNIS algorithm
weight: 39.61
last_reviewed_on: 2022-03-04
review_in: 6 months
---

# Use the IUNIS algorithm

In the Inferring User Needs by Information Scents (IUNIS) algorithm, the list of keywords that represent the user journeyâ€™s intent are driven by the:

- pages that a user visits in their journey
- possible pages that a user could have visited but did not

The IUNIS algorithm weights pages according to how rare those pages are across all user journeys in a specified time period.

The algorithm then weights the derived keywords by how important and unique those words are to the weighted pages compared to the rest of GOV.UK. The algorithm only keeps the top-weighted keywords.

To use IUNIS, you must create the:

- page term TF-IDF matrix
- page-access weight matrix
- step function alpha vector
- topology matrix

Then, run the IUNIS algorithm on the step-alpha function.

## Create the page term TF-IDF matrix

In information retrieval, [term frequency-inverse document frequency (TF-IDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a numerical statistic that shows how important a word is to a document.

The TF-IDF gives weight to words that are more important to documents, and less weight to common words such as "the" or "and".

How you create the page term TF-IDF matrix differs depending on whether you are using AWS Sagemaker or not. You can either:

- set the environment variables on your local machine, or
- use AWS Sagemaker to open a copy of the GOV.UK mirror

### Set the environment variables on your local machine

[Use `direnv`](/analysis/intent-detector/#load-the-secrets-environment-variable) to set the `DIR_MIRROR` environment variable in the `.envrc` file on your local machine.

When setting the `DIR_MIRROR` environment variable, use the folder path to your [downloaded copy of the GOV.UK mirror](/analysis/intent-detector/#download-a-copy-of-the-gov-uk-mirror).

You can now [go to the Jupyter notebooks directory](#go-to-the-jupyter-notebooks-directory.

### Use AWS Sagemaker to open a copy of the GOV.UK mirror

1. Open the [`govuk-intent-detector-page-term-tf-idf` AWS instance](https://eu-west-1.console.aws.amazon.com/sagemaker/home?region=eu-west-1#/notebook-instances/govuk-intent-detector-page-term-tf-idf).

    You must have the `govuk-datascienceusers` AWS IAM role to access this instance. [Contact the GOV.UK Data Labs team](mailto:gov.uk-data-labs@digital.cabinet-office.gov.uk) for more information.

1. There should be a copy of the GOV.UK mirror in the instance.

    If there is no copy of the mirror, or the copy is not the correct date, [download a new copy of the GOV.UK mirror to the AWS S3 bucket](/analysis/mirror/#copy-gov-uk-mirror-from-aws-s3-bucket).

1. In the AWS instance, select __Open Jupyter__ or __Open JupyterLab__ and run the `govuk-intent-detector/startup_script.ipynb`notebook.

1. Once the notebook has finished running, select __File > New... > Terminal__.

1. Run the following in the terminal to copy the GOV.UK mirror copy from the AWS S3 bucket to this AWS instance:

    ```
    cd /home/ec2-user/SageMaker/govuk-intent-detector
    aws s3 sync s3://govuk-data-infrastructure-integration/{YYYYMMDD}-govuk-production-mirror-replica ./govuk-production-mirror-replica
    ```

    `{YYYYMMDD}` is the date of the GOV.UK mirror copy in the AWS S3 bucket.

You can now [go to the Jupyter notebooks directory](#go-to-the-jupyter-notebooks-directory).

### Go to the Jupyter notebooks directory

1. Go to the root of the `govuk-intent-detector` repo directory on your local machine.

1. Run `jupyter notebook` in your command line to open Jupyter.

1. In Jupyter, go to the `notebooks/page_term_tfidf_matrix` directory.

### Produce a page-term TF-IDF matrix for the GOV.UK mirror

You can produce a page-term TF-IDF matrix for the GOV.UK mirror, or for the BigQuery data set.

To produce a page-term TF-IDF matrix for the GOV.UK mirror, you must run multiple Python notebooks in a certain order.

Change the `DATA_DATE` in each notebook before you run it.

Open each notebook in turn, and run all of the cells in that notebook.

Run the notebooks in the following order.

|Name|Purpose|Output location|
|:---|:---|:---|
|`002`|Gets the filepaths to every HTML file from the GOV.UK mirror.|The `govuk-intent-detector/data/interim` folder on your local machine.|
|`004b`|Parses all the valid HTML pages from notebook `002` to show only visible text.<br> Removes irrelevant HTML such as the header and banner.|The `govuk-intent-detector/data/interim` folder on your local machine.|
|`005`|Changes the outputs from notebook `004b`:<br> - Sets to lowercase<br> - Removes numbers and non-alpha symbols<br> - [Lemmatises](https://en.wikipedia.org/wiki/Lemmatisation) output using [WordNet parts of speech](https://wordnet.princeton.edu/documentation/wngloss7wn#:~:text=WordNet%20defines%20%22part%20of%20speech,Same%20as%20syntactic%20category%20.)<br> - Removes English stopwords|The `govuk-intent-detector/data/interim` folder on your local machine.|
|`006`|Generates the TF-IDF matrix|The `govuk-intent-detector/data/processed` folder on your local machine.|

### Produce a page-term TF-IDF matrix for BigQuery

You can produce a page-term TF-IDF matrix for the GOV.UK mirror, or for the BigQuery data set.

You should produce a page-term TF-IDF matrix for the BigQuery data set to cover pages that are not normally in the GOV.UK mirror, such as search API pages.

To produce a page-term TF-IDF matrix for the BigQuery data set, you must run multiple Python notebooks in a certain order.

Change the `DATA_DATE` in each notebook before you run it.

Open each notebook in turn, and run all of the cells in that notebook.

Run the notebooks in the following order.

|Name|Purpose|Output location|
|:---|:---|:---|
|`001`|Gets all visited GOV.UK pages from BigQuery for a specific day.|The `govuk-intent-detector/data/interim` folder on your local machine.|
|`002`|Gets the filepaths to every HTML file from the GOV.UK mirror.|The `govuk-intent-detector/data/interim` folder on your local machine.|
|`003`|Compares the outputs from `001` and `002` and web-scrapes GOV.UK for any missing pages.|The `govuk-intent-detector/data/interim` folder on your local machine.|
|`004`|Parses all the valid HTML pages from `001` to show only visible text. If a fragment (#) exists in the page path, the notebook only extracts the visible text at and after this fragment.|The `govuk-intent-detector/data/interim` folder on your local machine.|
|`005`|Changes the outputs from notebook `004b`:<br> - Sets to lowercase<br> - Removes numbers and non-alpha symbols<br> - [Lemmatises](https://en.wikipedia.org/wiki/Lemmatisation) output using [WordNet parts of speech](https://wordnet.princeton.edu/documentation/wngloss7wn#:~:text=WordNet%20defines%20%22part%20of%20speech,Same%20as%20syntactic%20category%20.)<br> - Removes English stopwords|The `govuk-intent-detector/data/interim` folder on your local machine.|
|`006`|Generates the TF-IDF matrix|The `govuk-intent-detector/data/processed` folder on your local machine.|

## Create the page access weight matrix

The page access TF-IDF weight matrix gives less weight to pages that are accessed by many users and are not relevant to the user's information needs. For example, the GOV.UK homepage.

### Definitions

The following definitions apply to this matrix:

- `user` is defined as either `sessionId` or `fullVisitorId`
- `pages` are the pages in the curated set of user journeys, otherwise known as `target urls`
- `term frequency` (TF) is the access frequency of the page by the given user
- `inverse document frequency` (IDF) is the ratio of total users to the number of distinct users who have accessed the given page

### Create the matrix

1. Go to the `intent-detector` repo folder on your local machine.

1. Run the [`make_user_page_access_matrix.py` Python script](https://github.com/alphagov/govuk-intent-detector/blob/main/src/make_data/make_user_page_access_matrix.py) to create the user-page access matrix.
___how? in jupyter?___
run `python src/make_data/make_user_page_access_matrix.py` in your command line

    Specify the following positional arguments in the script:
    - `--which-user` (optional, default is `sessionId`) states whether the user in the user-page access matrix is the `sessionId` or the `fullVisitorId`
    - `--start_date` (required) is the start date to query as a `YYYYMMDD` string, for example `20210428`
    - `--end_date` (required) is the end date to query as a `YYYYMMDD` string, for example `20210428`
    - `--tfidf-log` (optional, default is `1 : 'yes'`) states whether to use the log of the IDF in the TF-IDF calculation

    For example, to create a visitor page access TF-IDF matrix for 28 April 2021 using the raw IDF, go to the root-level of the `intent-detector` repo and run the following in your command line:
    ___terminal or command line?___

    ```
    python -m src.make_data.make_user_page_access_matrix --start-date "20210428" \
    --end-date "20210428" --which-user "fullVisitorId" --tfidf-log 0
    ```

### Outputs

When you create the page access TF-IDF matrix, the script outputs 2 `.parquet` files to the `intent-detector` repo on your local machine:

- the raw output table of the SQL query, saved in the `data/interim/` folder as `{start_date>}_{end_date}_{chosen_user}_page_access_raw_table.parquet`
- the user-target page access TF-IDF matrix, saved in the `data/interim/` folder as `{start_date>}_{end_date}_{chosen_user}_{tfidf_log}_page_access_matrix.parquet`

## Create the step function alpha vector

IUNIS - given this page that the user landed on, can you desscribe it using vocab from previous pages in that session
WUFIS - given this current page, can we say how likely it is that the user will visit n more pages?

WUFIS, sessions are likely to have 1 more page, less likely to have 2 more, unlikely to have 3 more and so on

the alpha dampening parameter reflects that likelihood for the n future pages in the session

vector is more than one number, but still only one parameter in the algorithms


___Assumptions and caveats here? https://github.com/alphagov/govuk-intent-detector/blob/main/docs/aqa/assumptions_caveats.md#tf-idf-in-user-page-access-matrix___

The script produces an alpha dampening parameter for the pages visited within a specified time window, on all of GOV.UK. This is a vector, beginning at 100%, and decreasing thereafter, for example 40%, then 20%. Each element represents the percentage of sessions that were n pages long or longer, so 100% of sessions were 1 page long or longer, 40% were two pages long or longer, etc.

The vector that is actually used is a modification of that. When the elements of alpha are applied, one after the other, their effect is the product of all the elements applied so far. In the example, the vector that would be used would be 100%, then 40%, then 50%, so that the cumulative products of that vector would be 100%, 40%, and 20%, which are the percentage of sessions that were 1 page long or longer, two pages long or longer, etc.

=n pages long | % of sessions | Alpha | Cumulative product of alpha ---------------|---------------|-------|---------------------------- 1 | 100% | 100 | 100% 2 | 40% | 40% | 40% 3 | 20% | 50% | 20%

___i have no idea what this means___

algorithm that gives a list of vocab that describes a page on gov.uk, some of that vocab is from the page itself, but augmented from other pages that users visit in the same session
Some sessions are long, and the pages at the end of the session may not be relevant to the pages at the beginning of the session
So for each page in the session, we give it a score to reflect how distant / removed it is from the page that we're interested in for that session
Use that score for those pages to choose what vocab to describe the page we're interested in
there are several different page scoring systems going on, this is one of them, the one that lowers the score of a word that is distant from the page we are interested in
snag: the score for a page is going to be applied several different times as the algorithm loops, so dampening factor has to be adjusted to account for the loops, so that the factor raised to the power of the loops of the algorithm, e.g. we want 50% to be the dampening factor, and there are 4 loops, so the dampening factor in the algorithm must be 1/16, as 1/16 to the power of 4 is 50%


### Definitions

The alpha dampening parameter is one of the component of the WUFIS and IUNIS algorithms to predict journeys and to infer intents. It makes more sense in WUFIS, as a kind of stopping rule for predicting a journey; if a journey is already longer than most journeys on GOV.UK, then a user is unlikely to follow any further links, nomatter how relevant -- therefore downweight those pages.

Given all sessions on GOV.UK between two dates, create a vector where the first element is the percentage of sessions 1-or-more pages long, the second element is the percentage of sessions 2-or-more pages long, and the nth element is the percentage of sessions n-or-more pages long.

___this is not a definition - what is this supposed to say???___

### Create the vector

the function that returns the dampening parameters that you want



Run the python script `src.make_data.make_alpha_step_function` to create the step-function alpha-values vector.

___where???___

The script allows for the following positional arguments:

Args:

``--alpha-start-date``: (required) start date to query as "YYYYMMDD" string (e.g "20210428")
``--alpha-end-date``:   (required) start date to query as "YYYYMMDD" string (e.g., "20210428")
For instance, to calculate the alpha vector for the PoC curated-jouneys pages based on sessions between 28-April-2021 to 04-May-2021 run in the root directory:

    python -m src.make_data.make_alpha_step_function --alpha-start-date "20210428" --alpha-end-date "20210428"

Outputs

Running the script => following output

A .parquet file data/interim/{START_DATE}_{END_DATE}_alpha_step_function.parquet, with the output table of the sql query. The table has five columns:

___where does this output to???___

numberPagesVisited integer; how many pages were visited in a session
frequencyPageCount integer; the number of sessions that visited that many pages
percentagePageCount float; the percentage of sessions that visited that many pages
cumulativePercentagePageCount float; the percentage of sessions that visited at least that many pages
relativePercentagePageCount float; of sessions that visited at least numberpagesVisited - 1, the percentage that also visited numberPagesVisited. When numberPagesVisited is 1, relativePercentagePageCount is also 1.

## Create the topology matrix

The topology matrix, also known as the directed adjacency matrix, represents how pages are connected to each other through hyperlink.

Two versions of the topology matrix have been generated:

For the pages in the curated set of user journeys.
For all the pages in the GOV.UK mirror.
The version for the curated set of journeys treats URLs with anchors as pages in their own right (an approach that is now deprecated), whereas the version for the GOV.UK mirror ignores anchors. See the assumptions and caveats.

___which version are we using now???___
document both

### Create the matrix (curated journeys)

At the root-level of this repository, open Jupyter in your terminal.

jupyter notebook



The notebook requires:

an internet connection
___do we really need to specify this???___
a .yaml file containing a list of "curated" GOV.UK page urls saved in `data/external/curated_journey_urls.yaml`
___where is this???___
The .yaml file should have a single key, target_urls, with the list of "curated" GOV.UK pages as its value.

duncan committed the yaml file, it exists already

For example, if the only curated pages are the GOV.UK homepage, and the main coronavirus page, the yaml file is:

```
target_urls:
  - /
  - /coronavirus
.
```
___Does the user need to change the curated list of pages?___



The notebook consists of two steps:

For the pages specified in the curated user-journey set (see .yaml file), download the (current time) GOV.UK HTML pages locally. For any pages specified with an anchor heading, the code automatically strips out all HTML content prior to this heading.

Generate the directed adjacency (topology) matrix and save it locally as a .pickle file to the data/interim/ folder.

___what am I doing and how am I doing it???___
___or is this what the notebook does when you run it?___
this is what the notebook does

Outputs
The HTML pages will be saved locally in the data/raw/html folder.
The topology matrix will be saved in the data/interim/ folder as {YYYYMMDD}_{HHMMSS}_govuk_topology_matrix.pickle

### Create the matrix (GOV.UK Mirror)

At the root-level of this repository, run the following command.

python -m src.make_data.generate_topology_matrix
Requirements
A copy of the GOV.UK production mirror.

___do you need the copy of the mirror if you use AWS sagemaker?___
just get a copy of the mirror

Outputs
Three files will be saved locally in the data/interim folder.

{YYYYMMDD}_{HHMMSS}_govuk_link_list.pickle") A dictionary keyed by URL, where each item contains a list of other URLs that are linked to.
{YYYYMMDD}_{HHMMSS}_govuk_topology_matrix.pickle") A scipy sparse matrix. The first dimension is the 'from' URL, and the second dimension is the 'to' URL.
{YYYYMMDD}_{HHMMSS}_govuk_vertex_url.pickle") A vector of URLs in the same order as each dimension of the topology matrix. This is used to slice the topology matrix for URLs of interest.
Slicing the topology matrix for a given URL
See commented code at the bottom of notebooks/generate_topology_matrix.py for an example.

## Run the IUNIS algorithm on the step-alpha function.

https://github.com/alphagov/govuk-intent-detector/blob/main/notebooks/proof_of_concept_alpha_step_function.py

root folder of repo

notebooks/proof_of_concept_alpha_step_function.py

output file

data/processed/poc_intents_step_alpha.csv


___what am i doing and how am i doing it???___
