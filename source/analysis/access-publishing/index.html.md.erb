---
title: Use the publishing database
weight: 39
last_reviewed_on: 2021-11-04
review_in: 6 months
---

# Use the GOV.UK publishing database

The GOV.UK publishing database is a PostgreSQL database that keeps a copy of every version of every page ever drafted or published on GOV.UK.

You can connect to and use the publishing database instead of [downloading the GOV.UK mirror to your local machine](/analysis/mirror/). You can use the publishing database to compute intents.

If you only want to analyse the most recent version of every page that is currently published, you should [use the Content Store](/analysis/content-store/).

The data community does not currently have access to the production environment of the publishing database, so we access the database using the integration environment.

Instead of connecting to the integration environment database, you can download a backup of the publishing database to your local machine if you need to:

- run resource-intensive queries on the database, as these queries may affect live services that also use the database
- create your own tables within the database, as you only have read access to the integration environment database

## Connect to the publishing database

Before you start, you must:

- have access to AWS
- install the GDS command line tools
- connect to the GDS VPN
- get Secure Shell (SSH) access to the integration environment

See the [Get started on GOV.UK developer documentation](https://docs.publishing.service.gov.uk/manual/get-started.html) for more information on how to do this.

1. Run the following in the command line to connect to an AWS instance in the integration environment:

    ```
    USER=<FIRSTNAMELASTNAME>; gds govuk connect --environment integration ssh publishing_api_db_admin
    ```

    `<FIRSTNAMELASTNAME>` must be the same user that you created to SSH into integration.

    You do not need to include `USER=<FIRSTNAMELASTNAME>;` if the username for your machine is the same as the SSH integration user.

1. Open a PostgreSQL terminal to the `postgresql-primary` host as the `aws_db_admin` user, and connect to the publishing API database:

    ```
    sudo psql -U aws_db_admin -h publishing-api-postgres --no-password -d publishing_api_production
    ```

    This database is named `production`, but it is not in the production environment.

You can now run queries on the integration environment publishing database.

### Download query results from the publishing database

Once you have run queries on the publishing database, download the results of those queries to your local machine by either:

- printing the query results in the command line, and sending those results to a file on your local machine
- writing the query results to a file in the `db_admin` server, compressing the file, and then downloading it to your local machine

#### Print query results in the command line

Run the following in the command line to print a query's results and send those results to a file on your local machine:

```
USER=<FIRSTNAMELASTNAME>; gds govuk connect --environment integration ssh db_admin sudo "psql -U aws_db_admin -h postgresql-primary --no-password -d publishing_api_production -c \"<SQL-QUERY>\"" > <FILENAME>
```

where:

- `<SQL-QUERY>` is a SQL query
- `<FILENAME>` is the name of a file to write the query results to

#### Write query results to a server file

The `gds govuk connect` command enables you to fetch a file from a server and save it locally.

1. Run the following in the command line to execute the query, and save the results to a file in the `db_admin` server:

    ```
    USER=<FIRSTNAMELASTNAME>; gds govuk connect --environment integration ssh db_admin:1 sudo "psql -U aws_db_admin -h postgresql-primary --no-password -d publishing_api_production -c \"\\copy ($(<<QUERY-FILE>)) TO '/tmp/<FILENAME>.csv' WITH CSV HEADER\""
    ```

    where:
    - `<QUERY-FILE>` is a file on your local machine that contains an SQL query. Keep the first `<` in `<<QUERY-FILE>`, for example `<myfile.sql`.
    - `<FILENAME>` is a file in the server that contains the query results.

    The `db_admin` server does not run the database itself.

1. Compress the results file by running:

    ```
    gds govuk connect --environment integration ssh db_admin zip "/tmp/<FILENAME>.zip /tmp/<FILENAME>.csv"
    ```

    where `<FILENAME>` is the same as the previous step, a file in the server that contains the query results.

1. Use `scp-pull` to fetch the file to your local machine.

    ```
    gds govuk connect --environment integration scp-pull db_admin:1 /tmp/<FILENAME>.zip <DOWNLOAD-LOCATION>
    ```
    where:
    - `<FILENAME>` is the same as the previous step, a file in the server that contains the query results.
    - `<DOWNLOAD-LOCATION>` is the location you want to download the backup file to on your local machine.

    The `:1` in `db_admin:1` guarantees that `scp-pull` will connect to the same machine as `ssh`.

    The `1` chooses the first available machine. There is only one machine in the integration environment, so `1` is guaranteed to exist.

### Upload the downloaded results into BigQuery

When you have downloaded your query results from the publishing database, you can then upload those results into BigQuery for analysis.

1. Run the following in the command line to convert the downloaded file to a parquet file using the Pandas package:

    ```
    import pandas as pd
    df = pd.read_csv("<FILENAME>.zip")

    # gzip compression is slower than snappy (default) but creates a much smaller file (123MB rather than 302MB)
    df.to_parquet("<FILENAME>.parquet", compression='gzip')
    ```

    where `<FILENAME>` is the name of the zipped CSV file and of the parquet file that you want to convert it into.

1. Upload the parquet file to BigQuery using the Google Cloud SDK command `bq` in the command line:

    ```
    bq load --source_format=PARQUET <DATASET>.<TABLE> <FILENAME>.parquet
    ```

    where:
    - `<FILENAME>` is the name of the zipped CSV file and of the parquet file to convert the CSV file into
    - `<DATASET>` and `<TABLE>` are where to put the data in BigQuery

You have now uploaded your query results into BigQuery for analysis.

## Download a backup of the publishing database

Instead of connecting to the integration environment database, you can download a backup of the publishing database to your local machine if you need to:

- run resource-intensive queries on the database, as these queries may affect live services that also use the database
- create your own tables within the database, as you only have read access to the integration environment database

Before you start, you must:

- have [access to AWS](https://docs.publishing.service.gov.uk/manual/get-started.html#7-get-aws-access)
- install the [GDS command line tools](https://docs.publishing.service.gov.uk/manual/get-started.html#3-install-gds-command-line-tools)

You do not need the GDS VPN or Secure Shell (SSH) access to the integration environment.

### Download the database backup

1. The filename of the latest database backup in AWS S3 varies, depending on the date and time the backup was created. Run the following command to find out the filename:

    ```sh
    gds aws govuk-integration-readonly aws s3 ls s3://govuk-integration-database-backups/publishing-api-postgres/
    ```

1. Download the backup of the publishing API database:

    ```sh
    gds aws govuk-integration-readonly aws s3 cp s3://govuk-integration-database-backups/publishing-api-postgres/<FILENAME> <DOWNLOAD-LOCATION>
    ```
    where:
    - `<YYYY-MM-DD>T<HH:MM:SS>` is the filename of the backup database
    - `<DOWNLOAD-LOCATION>` is the location you want to download the backup file to on your local machine

### Open the database on your local machine

To work with the database backup on your local machine, you must open the backup on a database system.

The following content assumes you’re using [PostgreSQL](https://www.postgresql.org/) as your database system.

1. Run the following command to create a PostgreSQL database user called `publishing_api`:

    ```sh
    sudo -u postgres createuser -s publishing_api
    ```
1. Load the data from the backup database into a new PostgreSQL database called `publishing_api_production`:

    ```sh
    pg_restore --verbose --create --jobs=4 --dbname=postgres <DOWNLOAD-LOCATION>
    ```
    where:
    - `--jobs=4` is the number of CPU threads that will be used.  You can change this to suit your device.
    - `<DOWNLOAD-LOCATION>` is the location that you downloaded the backup file to on your local machine.

When your command line becomes responsive and you can enter commands again, this means you’ll have successfully created a database named `publishing_api_production` that contains the publishing API backup data.

### Run queries in the database

1. Run the following command to start using the database.

    ```sh
    psql -d publishing_api_production
    ```

    You should see the following output:

    ```text
    psql (13.6)
    Type "help" for help.

    publishing_api_production=#
    ```

1. Enter a query to start using the database. For example:

    ```sql
    SELECT title FROM editions LIMIT 5;
    ```

You can now work with this database in PostgreSQL. To learn more about using PostgreSQL, [read the PostgreSQL documentation](https://www.postgresql.org/docs/).

## Run queries on the database

The Publishing API database does not specifically refer to GOV.UK pages.

The database contains records of URLs that users visit.

You can run queries on these records. However, the results you get back will differ depending on whether the page associated with the record are:

- fully represented by a URL record
- fully represented by a part of a URL record
- partly represented by a URL record
- not represented by a URL record

### Run queries on pages fully represented by a URL record

Every URL record in the Publishing API database begins with `https://www.gov.uk`. For example, `https://www.gov.uk/transport`.

These URLs are in the `base_path` column in the database, excluding `https://www.gov.uk`. For example, the base_path of `https://www.gov.uk/transport` is `/transport`.

To find records about these types of pages, you run the query `select * from editions where base_path = <BASE_PATH>;`.

For example, `select * from editions where base_path = '/transport';`

### Run queries on pages fully represented by part of a URL record

Some URL records in the Publishing API database describe more than one URL. So those pages are fully represented by a part of a URL record.

The URL is made up of the `base_path` and the last part of the URL, known as the URL slug.

The slug is defined in the JSON data in the `details` column of the database.

For example, the page at `https://www.gov.uk/honours` has a contents list, where each heading is a link to another URL, such as `https://www.gov.uk/honours/honours-lists`.

In this example, the `base_path` is `honours`, and the slug is `/honours-lists`.

There are two queries you can run on this type of page.

You can run the following query to count how many of this type of page are currently published:

```
Select document_type, count(*) from editions where state=’published’ group by document_type order by document_type;
```

You can also run the following query to check which document types these pages and URLs fit into:

```
with samples as (
	select distinct on (document_type)
		document_type,
		base_path,
		details
	from editions
	where state='published'
	order by document_type, updated_at desc
)
select
	document_type
from samples
where details ? 'parts'
order by document_type;
```

Currently, running this query returns the document types `guide` and `travel_advice`.

#### Extracting the full URLs from a record that represents multiple URLs

You can extract the full URL of these types of pages, so that you have one record for each full URL.

You might need to do this to link a record of a page view from Google Analytics to a record of the content of the page in the Publishing API database.

Run the following query to extract each full URL into a separate record.

```
select
  base_path,
  updated_at,
  CONCAT(
    base_path,
    CASE WHEN parts.value->>'slug' IS NOT NULL
    THEN '/'
    ELSE ''
    END,
    PARTS.VALUE->>'slug'
  ) AS pagePath
  FROM editions
  LEFT JOIN LATERAL
  json_array_elements(details::json->'parts') AS parts
  ON true
  WHERE base_path = '<BASE-PATH>'
  ORDER BY updated_at
```
`'<BASE-PATH>'` is the base_path you want to extract full URLs from, for example, `'/honours'`.

This query does not affect records that already represent the full URL in their `base_path`.

### Run queries on pages partly represented by a URL record

Some pages are represented by URL records in the database, but those pages' content is not included in those records. For example, `https://www.gov.uk/bank-holidays`.

Another example of this type of page is a page with dynamic content. Dynamic content is created automatically in response to information provided by the user.

For example, a search results page at `https://gov.uk/search/all`.

In this example, you can run the following query on the search results page:

```
select * from editions where base_path = '/search' limit 1;
```

### Run queries on pages not represented by a URL record

The following types of pages are not represented by a URL record:

- pages with dynamic content
- pages with no content
- pages that do not exist

#### Pages with dynamic content

Dynamic content is created automatically in response to information provided by the user.

An example of this type of page is `https://gov.uk/towing-rules/y/car-or-light-vehicle`, which is a smart answer.

In this example, you can run the following query on the smart answer page:

```
select * from editions where base_path = '/towing-rules/y/car-or-light-vehicle' limit 1;
```

#### Pages with no content

Pages with no content are not represented by a URL record.

An example of this type of page is `https://www.gov.uk/random`, which has no content and redirects to a page chosen at random

In this example, you can run the following query on a page with no content:

```
select * from editions where base_path = '/random' limit 1;
```

#### Pages that do not exist

Pages that do not exist are not represented by a URL record.

URLs of non-existent pages often appear in Google Analytics and server logs. Sometimes these pages are typos.

We have used some common typos to create pages that redirect to the correctly spelled equivalent page.

In this example, you can run the following query on a page with a typo:

```
select *, redirects from editions where base_path = '/view-driving-license' limit 1;
```
