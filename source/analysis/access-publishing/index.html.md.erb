---
title: Access the publishing database
weight: 39
last_reviewed_on: 2021-11-04
review_in: 6 months
---

# Access the GOV.UK publishing database

The GOV.UK publishing database is a PostgreSQL database that keeps a copy of every version of every page published on GOV.UK.

You can connect to and access the publishing database instead of [downloading the GOV.UK mirror to your local machine](/analysis/mirror/). You can use the publishing database to compute intents.

The data community does not currently have access to the production environment of the publishing database, so we access the database using the integration environment.

Instead of connecting to the integration environment database, you can download a backup of the publishing database to your local machine if you need to:

- run resource-intensive queries on the database, as these queries may affect live services that also use the database
- create your own tables within the database, as you only have read access to the integration environment database

## Connect to the publishing database

Before you start, you must:

- have access to AWS
- install the GDS command line tools
- connect to the GDS VPN
- get Secure Shell (SSH) access to the integration environment

See the [Get started on GOV.UK developer documentation](https://docs.publishing.service.gov.uk/manual/get-started.html) for more information on how to do this.

1. Run the following in the command line to connect to an AWS instance in the integration environment:

    ```
    USER=<FIRSTNAMELASTNAME>; gds govuk connect --environment integration ssh publishing_api_db_admin
    ```

    `<FIRSTNAMELASTNAME>` must be the same user that you created to SSH into integration.

    You do not need to include `USER=<FIRSTNAMELASTNAME>;` if the username for your machine is the same as the SSH integration user.

1. Open a PostgreSQL terminal to the `postgresql-primary` host as the `aws_db_admin` user, and connect to the publishing API database:

    ```
    sudo psql -U aws_db_admin -h publishing-api-postgres --no-password -d publishing_api_production
    ```

    This database is named `production`, but it is not in the production environment.

You can now run queries on the integration environment publishing database.

### Download query results from the publishing database

Once you have run queries on the publishing database, download the results of those queries to your local machine by either:

- printing the query results in the command line, and sending those results to a file on your local machine
- writing the query results to a file in the `db_admin` server, compressing the file, and then downloading it to your local machine

#### Print query results in the command line

Run the following in the command line to print a query's results and send those results to a file on your local machine:

```
USER=<FIRSTNAMELASTNAME>; gds govuk connect --environment integration ssh db_admin sudo "psql -U aws_db_admin -h postgresql-primary --no-password -d publishing_api_production -c \"<SQL-QUERY>\"" > <FILENAME>
```

where:

- `<SQL-QUERY>` is a SQL query
- `<FILENAME>` is the name of a file to write the query results to

#### Write query results to a server file

The `gds govuk connect` command enables you to fetch a file from a server and save it locally.

1. Run the following in the command line to execute the query, and save the results to a file in the `db_admin` server:

    ```
    USER=<FIRSTNAMELASTNAME>; gds govuk connect --environment integration ssh db_admin:1 sudo "psql -U aws_db_admin -h postgresql-primary --no-password -d publishing_api_production -c \"\\copy ($(<<QUERY-FILE>)) TO '/tmp/<FILENAME>.csv' WITH CSV HEADER\""
    ```

    where:
    - `<QUERY-FILE>` is a file on your local machine that contains an SQL query. Keep the first `<` in `<<QUERY-FILE>`, for example `<myfile.sql`.
    - `<FILENAME>` is a file in the server that contains the query results.

    The `db_admin` server does not run the database itself.

1. Compress the results file by running:

    ```
    gds govuk connect --environment integration ssh db_admin zip "/tmp/<FILENAME>.zip /tmp/<FILENAME>.csv"
    ```

    where `<FILENAME>` is the same as the previous step, a file in the server that contains the query results.

1. Use `scp-pull` to fetch the file to your local machine.

    ```
    gds govuk connect --environment integration scp-pull db_admin:1 /tmp/<FILENAME>.zip <DOWNLOAD-LOCATION>
    ```
    where:
    - `<FILENAME>` is the same as the previous step, a file in the server that contains the query results.
    - `<DOWNLOAD-LOCATION>` is the location you want to download the backup file to on your local machine.

    The `:1` in `db_admin:1` guarantees that `scp-pull` will connect to the same machine as `ssh`.

    The `1` chooses the first available machine. There is only one machine in the integration environment, so `1` is guaranteed to exist.

### Upload the downloaded results into BigQuery

When you have downloaded your query results from the publishing database, you can then upload those results into BigQuery for analysis.

1. Run the following in the command line to convert the downloaded file to a parquet file using the Pandas package:

    ```
    import pandas as pd
    df = pd.read_csv("<FILENAME>.zip")

    # gzip compression is slower than snappy (default) but creates a much smaller file (123MB rather than 302MB)
    df.to_parquet("<FILENAME>.parquet", compression='gzip')
    ```

    where `<FILENAME>` is the name of the zipped CSV file and of the parquet file that you want to convert it into.

1. Upload the parquet file to BigQuery using the Google Cloud SDK command `bq` in the command line:

    ```
    bq load --source_format=PARQUET <DATASET>.<TABLE> <FILENAME>.parquet
    ```

    where:
    - `<FILENAME>` is the name of the zipped CSV file and of the parquet file to convert the CSV file into
    - `<DATASET>` and `<TABLE>` are where to put the data in BigQuery

You have now uploaded your query results into BigQuery for analysis.

## Download a backup of the publishing database

Instead of connecting to the integration environment database, you can download a backup of the publishing database to your local machine if you need to:

- run resource-intensive queries on the database, as these queries may affect live services that also use the database
- create your own tables within the database, as you only have read access to the integration environment database

Before you start, you must:

- have [access to AWS](https://docs.publishing.service.gov.uk/manual/get-started.html#7-get-aws-access)
- install the [GDS command line tools](https://docs.publishing.service.gov.uk/manual/get-started.html#3-install-gds-command-line-tools)

You do not need the GDS VPN or Secure Shell (SSH) access to the integration environment.

### Download the database backup

1. The filename of the latest database backup in AWS S3 varies, depending on the date and time the backup was created. Run the following command to find out the filename:

    ```sh
    gds aws govuk-integration-readonly aws s3 ls s3://govuk-integration-database-backups/publishing-api-postgres/
    ```

1. Download the backup of the publishing API database:

    ```sh
    gds aws govuk-integration-readonly aws s3 cp s3://govuk-integration-database-backups/publishing-api-postgres/<FILENAME> <DOWNLOAD-LOCATION>
    ```
    where:
    - `<YYYY-MM-DD>T<HH:MM:SS>` is the filename of the backup database
    - `<DOWNLOAD-LOCATION>` is the location you want to download the backup file to on your local machine

### Open the database on your local machine

To work with the database backup on your local machine, you must open the backup on a database system.

The following content assumes you’re using [PostgreSQL](https://www.postgresql.org/) as your database system.

1. Run the following command to create a PostgreSQL database user called `publishing_api`:

    ```sh
    sudo -u postgres createuser -s publishing_api
    ```
1. Load the data from the backup database into a new PostgreSQL database called `publishing_api_production`:

    ```sh
    pg_restore --verbose --create --jobs=4 --dbname=postgres <DOWNLOAD-LOCATION>
    ```
    where:
    - `--jobs=4` is the number of CPU threads that will be used.  You can change this to suit your device.
    - `<DOWNLOAD-LOCATION>` is the location that you downloaded the backup file to on your local machine.

When your command line becomes responsive and you can enter commands again, this means you’ll have successfully created a database named `publishing_api_production` that contains the publishing API backup data.

### Run queries in the database

1. Run the following command to start using the database.

    ```sh
    psql -d publishing_api_production
    ```

    You should see the following output:

    ```text
    psql (13.6)
    Type "help" for help.

    publishing_api_production=#
    ```

1. Enter a query to start using the database. For example:

    ```sql
    SELECT title FROM editions LIMIT 5;
    ```

You can now work with this database in PostgreSQL. To learn more about using PostgreSQL, [read the PostgreSQL documentation](https://www.postgresql.org/docs/).
